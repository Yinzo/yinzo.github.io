<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[雪地]]></title>
  <link href="http://yinzo.github.io/atom.xml" rel="self"/>
  <link href="http://yinzo.github.io/"/>
  <updated>2016-01-29T16:03:32+08:00</updated>
  <id>http://yinzo.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Notes for "ICMLC2009-FabioRoli.pdf"]]></title>
    <link href="http://yinzo.github.io/14540480758140.html"/>
    <updated>2016-01-29T14:14:35+08:00</updated>
    <id>http://yinzo.github.io/14540480758140.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Understanding:</h2>

<h3 id="toc_1">1. What is adversarial classification? Basic concepts and motivations</h3>

<p>The Classifier which put the adversary actions into account. It can develop according to the adversary actions.</p>

<p>Its motivations is that the classical model cannot perform well in adversarial environments. Because the classical model is build and set up base on the random noise, it&#39;s also use for normal random noise environment. But in adversarial environment, the noise it face is adversarial noise, which is generated by adversary on purpose.</p>

<h4 id="toc_2">Points:</h4>

<ul>
<li>The classical model does not fit well with adversarial tasks</li>
<li>We need adversary-aware classification models</li>
</ul>

<h3 id="toc_3">2. Adversary-aware classification</h3>

<p>The classical model is build for the normal random noise. When facing the adversarial noise, its performance would be <em>significantly degrade</em>, while the adversary-aware model works better.</p>

<h4 id="toc_4">Points:</h4>

<ul>
<li>Classification algorithms should take into account the adversary</li>
<li>Classifier should be adaptive by exploiting any feedback that they can get about adversary&#39;s moves</li>
</ul>

<h3 id="toc_5">3. Vulnerability assessment in pattern classification systems</h3>

<p>The hardness of evading the spam classifier is regard as the judging standard of vulnerability assessment in pattern classification systems, which use the <em>minimum numbers of features that needs to be modified to evade classifier</em> to calculate the score.</p>

<h4 id="toc_6">Points:</h4>

<ul>
<li>Classification accuracy is not everything in adversarial tasks</li>
<li>Designer should maximize both accuracy and hardness of evasion of the classifier</li>
</ul>

<h3 id="toc_7">4. Defense strategies</h3>

<p>Basically, the main strategies is to make the evasion too costly for the adversary. We normally implement this by using multiple classifiers with different detect strategies, to add up the cost of evasion.</p>

<p>Also, for the close-source classifiers, we can make the classifiers activate randomly, which make the adversary needs to do much more detection ( \( \Theta(n) = 2^n \) ) , to figure out how the classifier work.</p>

<h4 id="toc_8">Points:</h4>

<ul>
<li>So for we have some state-of-the-art works on defense strategies against specific attacks for specific applications</li>
<li>Defense strategies against different types of attacks for different applications are a matter of on-going research</li>
</ul>

<h3 id="toc_9">5. Conclusions and open research issues</h3>

<p>There is few adversary-aware model, so does the general-purpose methods for vulnerability assessment and defenses against a variety of attacks.</p>

<ul>
<li>models base on various scenes</li>
<li>integrated strategies for defense and vulnerability assessment</li>
<li>put the test into reality but not simply static data sets</li>
</ul>

]]></content>
  </entry>
  
</feed>
