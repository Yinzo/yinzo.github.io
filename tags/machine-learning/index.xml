<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on 雪地</title>
    <link>https://yinzo.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on 雪地</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn-zh</language>
    <lastBuildDate>Wed, 07 Jun 2017 14:35:00 +0800</lastBuildDate>
    
	<atom:link href="https://yinzo.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CS229 学习笔记 Part 3</title>
      <link>https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-3/</link>
      <pubDate>Wed, 07 Jun 2017 14:35:00 +0800</pubDate>
      
      <guid>https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-3/</guid>
      <description>CS229 对于 SVM 的理论解释是我学习到的最详细也是最好的一份资料了，对比对象有周志华《机器学习》、《机器学习实战》、Coursera 上的 Machine Learning 等。相当推荐学习 CS229。
分类间隔 (Margin) 和 SVM 的优化目标『最大化分类间隔』这里就不多说了，很好理解，主要还是记录 CS229 中学到的新内容。一个数据点离分类边界 (decision boundary) 越远，则确信度越高。我们的优化目标也相当于寻找一个远离所有数据点的分类边界，当然，前提是这个分类边界得到的分类都正确。
SVM 的一些特殊定义也提及一下，
 $y$ 的取值不是 ${0,1}$ 而是 ${-1,1}$。 假设函数 $h_{w,b}(x) = g(w^Tx+b)$ 中，我们把截距项单独写出来，便与后续的计算。 我们的分类器输出结果会直接是 1 或 -1，不像 Logistic 回归那样先输出 $y$ 是某一类的概率。  函数间隔 $\hat{\gamma}$ 的定义如下 $$\hat{\gamma}^{(i)} = y^{(i)}(w^Tx+b)$$
$$\hat{\gamma} = \min_{i=1,\cdots,m} \hat{\gamma}^{(i)}$$
函数间隔，是所有数据点的函数输出中的最小值，函数间隔越大，说明这个点分类的自信越高。但是可以发现，我们等比例放大参数 $w$ 和 $b$ 的数值大小，可以使得函数间隔变大，并且分类间隔直线的位置并不会移动。于是我们又定义了几何间隔
注意图中的点 A，我们需要求 A 到分类边界的距离 $\gamma^{(i)}$，就是我们现在需要求的值。
因为 A 代表着 $x^{(i)}$， 所以我们可以得到点 B 的公式为 $x^{(i)} - \gamma^{(i)} \cdot w/|w|$，并且点 B 在分类边界上，我们有 $w^Tx+b=0$，因此</description>
    </item>
    
    <item>
      <title>CS229 学习笔记 Part 2</title>
      <link>https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-2/</link>
      <pubDate>Wed, 07 Jun 2017 00:20:00 +0800</pubDate>
      
      <guid>https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-2/</guid>
      <description>对于一个分类任务，判别式和生成式分别代表了两种不同的思路：
通过直接从输入数据中学习，得到一个『特定输入对应的实际类别』的概率模型，模型的参数为 $\theta$ 。即学习建模 $p(y\mid x)$
通过对每一个类进行建模，然后就可以通过条件概率算出输入的数据更可能由哪一类生成。即学习建模 $p(x\mid y)$ 和 $p(y)$ ，然后计算 $$\arg\max\limits_y\frac{p(x \mid y)p(y)}{p(x)}$$
并且实际计算中，分母 $p(x)$ 并不会影响各个类别概率的排序，所以最终简化成 $$\arg\max\limits_y p(x \mid y)p(y)​$$
作为生成式模型的第一个例子，它假设数据的分布 $p(x\mid y)$ 是多元高斯分布 (multivariate normal distribution)，分类结果为二分类，即 $y \sim \mathrm{Bernoulli}(\phi)$。
根据生成式模型的思路，它通过训练数据，计算出两个类的隐含分布——多元高斯分布的参数 $\mu_0, \mu_1,\Sigma$ （需要注意的是，这里对于两个多元正态分布的 $\Sigma$，我们使用的是一个公共的参数，也就是我们假设两个分布的『形状』是一样的），以及对于分类结果的伯努利分布参数 $\phi$
根据定义，我们可以得到以下模型
$$p(y) = \phi^y(1-\phi)^{1-y}$$
$$p(x\mid y=0) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/ 2}} \exp\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right)$$
$$p(x\mid y=1) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/ 2}} \exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)$$
接下来开始估计各个参数的值。我们使用一个新的似然函数 Joint likelihood
$$\ell(\phi, \mu_0, \mu_1, \Sigma) = log \prod^m_{i=1} p(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma)$$</description>
    </item>
    
    <item>
      <title>CS229 学习笔记 Part 1</title>
      <link>https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-1/</link>
      <pubDate>Fri, 12 May 2017 23:14:00 +0800</pubDate>
      
      <guid>https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-1/</guid>
      <description>此笔记为我的 CS229 的学习笔记之一，由 Andrew Ng 的 CS229 Lecture notes 和 课堂录像整理而来。用于记录所学到的内容。记录顺序重新编排过，并非是课程原本的教学顺序，并且省略了课程中的一些推导过程，所以适合学习后整理备忘使用，不适合用于同步辅助学习。
广义线性模型是所学到的 Linear Regression 以及 Logistic Regression 的推广形式（更准确的说，这两种模型都属于 GLM 的特殊情况）。它有三个关键假设(Assumptions)构成:
 $y \mid x;\theta\sim ExponentialFamily(\eta)$ ：对于固定的参数 $\theta$ 以及给定 $x$， $y$ 的分布服从某一指数分布族（如高斯分布、伯努利分布、Softmax分布） 对于给定的 $x$ ，目标是预测 $T(y)$ 的值。换一种说法就是，我们定义假设函数 $h(x) = E[y\mid x]$ natural parameter $\eta$ 和 输入 $x$ 是线性相关的， $\eta = \theta^ \mathrm{ T } x$ （其中，当输入 $x$ 和 $\eta$ 是向量的时候， $\eta_i = \theta_i^ \mathrm{T}x$）  以上三个假设，一般只有第一个需要我们决定所使用的分布，其他两个假设都是直接定义。关键的地方来了，通过选择不同的_指数分布族_分布，我们能够得到不同的模型：
 高斯分布，则得到 Linear Regression 伯努利分布，则得到 Logistic Regression Softmax 分布，得到 Softmax Regression  其中，Lenear Regression 为回归模型 (regression)， Logistic Regression 和 Softmax Regression 都是分类模型 (classification)。</description>
    </item>
    
  </channel>
</rss>