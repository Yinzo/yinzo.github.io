<!DOCTYPE html>
<html lang="cn-zh">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta itemprop="name" content="CS229 学习笔记 Part 1">
<meta itemprop="description" content="此笔记为我的 CS229 的学习笔记之一，由 Andrew Ng 的 CS229 Lecture notes 和 课堂录像整理而来。用于记录所学到的内容。记录顺序重新编排过，并非是课程原本的教学顺序，并且省略了课程中的一些推导过程，所以适合学习后整理备忘使用，不适合用于同步辅助学习。
广义线性模型是所学到的 Linear Regression 以及 Logistic Regression 的推广形式（更准确的说，这两种模型都属于 GLM 的特殊情况）。它有三个关键假设(Assumptions)构成:
 $y \mid x;\theta\sim ExponentialFamily(\eta)$ ：对于固定的参数 $\theta$ 以及给定 $x$， $y$ 的分布服从某一指数分布族（如高斯分布、伯努利分布、Softmax分布） 对于给定的 $x$ ，目标是预测 $T(y)$ 的值。换一种说法就是，我们定义假设函数 $h(x) = E[y\mid x]$ natural parameter $\eta$ 和 输入 $x$ 是线性相关的， $\eta = \theta^ \mathrm{ T } x$ （其中，当输入 $x$ 和 $\eta$ 是向量的时候， $\eta_i = \theta_i^ \mathrm{T}x$）  以上三个假设，一般只有第一个需要我们决定所使用的分布，其他两个假设都是直接定义。关键的地方来了，通过选择不同的_指数分布族_分布，我们能够得到不同的模型：
 高斯分布，则得到 Linear Regression 伯努利分布，则得到 Logistic Regression Softmax 分布，得到 Softmax Regression  其中，Lenear Regression 为回归模型 (regression)， Logistic Regression 和 Softmax Regression 都是分类模型 (classification)。">
<meta itemprop="datePublished" content="2017-05-12T23:14:00&#43;08:00" />
<meta itemprop="dateModified" content="2017-05-12T23:14:00&#43;08:00" />
<meta itemprop="wordCount" content="234">



<meta itemprop="keywords" content="Machine Learning," /><meta property="og:title" content="CS229 学习笔记 Part 1" />
<meta property="og:description" content="此笔记为我的 CS229 的学习笔记之一，由 Andrew Ng 的 CS229 Lecture notes 和 课堂录像整理而来。用于记录所学到的内容。记录顺序重新编排过，并非是课程原本的教学顺序，并且省略了课程中的一些推导过程，所以适合学习后整理备忘使用，不适合用于同步辅助学习。
广义线性模型是所学到的 Linear Regression 以及 Logistic Regression 的推广形式（更准确的说，这两种模型都属于 GLM 的特殊情况）。它有三个关键假设(Assumptions)构成:
 $y \mid x;\theta\sim ExponentialFamily(\eta)$ ：对于固定的参数 $\theta$ 以及给定 $x$， $y$ 的分布服从某一指数分布族（如高斯分布、伯努利分布、Softmax分布） 对于给定的 $x$ ，目标是预测 $T(y)$ 的值。换一种说法就是，我们定义假设函数 $h(x) = E[y\mid x]$ natural parameter $\eta$ 和 输入 $x$ 是线性相关的， $\eta = \theta^ \mathrm{ T } x$ （其中，当输入 $x$ 和 $\eta$ 是向量的时候， $\eta_i = \theta_i^ \mathrm{T}x$）  以上三个假设，一般只有第一个需要我们决定所使用的分布，其他两个假设都是直接定义。关键的地方来了，通过选择不同的_指数分布族_分布，我们能够得到不同的模型：
 高斯分布，则得到 Linear Regression 伯努利分布，则得到 Logistic Regression Softmax 分布，得到 Softmax Regression  其中，Lenear Regression 为回归模型 (regression)， Logistic Regression 和 Softmax Regression 都是分类模型 (classification)。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-1/" />
<meta property="article:published_time" content="2017-05-12T23:14:00+08:00" />
<meta property="article:modified_time" content="2017-05-12T23:14:00+08:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CS229 学习笔记 Part 1"/>
<meta name="twitter:description" content="此笔记为我的 CS229 的学习笔记之一，由 Andrew Ng 的 CS229 Lecture notes 和 课堂录像整理而来。用于记录所学到的内容。记录顺序重新编排过，并非是课程原本的教学顺序，并且省略了课程中的一些推导过程，所以适合学习后整理备忘使用，不适合用于同步辅助学习。
广义线性模型是所学到的 Linear Regression 以及 Logistic Regression 的推广形式（更准确的说，这两种模型都属于 GLM 的特殊情况）。它有三个关键假设(Assumptions)构成:
 $y \mid x;\theta\sim ExponentialFamily(\eta)$ ：对于固定的参数 $\theta$ 以及给定 $x$， $y$ 的分布服从某一指数分布族（如高斯分布、伯努利分布、Softmax分布） 对于给定的 $x$ ，目标是预测 $T(y)$ 的值。换一种说法就是，我们定义假设函数 $h(x) = E[y\mid x]$ natural parameter $\eta$ 和 输入 $x$ 是线性相关的， $\eta = \theta^ \mathrm{ T } x$ （其中，当输入 $x$ 和 $\eta$ 是向量的时候， $\eta_i = \theta_i^ \mathrm{T}x$）  以上三个假设，一般只有第一个需要我们决定所使用的分布，其他两个假设都是直接定义。关键的地方来了，通过选择不同的_指数分布族_分布，我们能够得到不同的模型：
 高斯分布，则得到 Linear Regression 伯努利分布，则得到 Logistic Regression Softmax 分布，得到 Softmax Regression  其中，Lenear Regression 为回归模型 (regression)， Logistic Regression 和 Softmax Regression 都是分类模型 (classification)。"/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>CS229 学习笔记 Part 1</title>
	<link rel="stylesheet" href="https://yinzo.github.io/css/style.min.657bcb7af31123e4156b1a3d2ff60a636717e54ead74f882136b5114cf72b55e.css" integrity="sha256-ZXvLevMRI+QVaxo9L/YKY2cX5U6tdPiCE2tRFM9ytV4=" crossorigin="anonymous">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp faster">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://yinzo.github.io/">雪地</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://yinzo.github.io/posts/">Posts</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<button id="toc-btn" class="hdr-btn desktop-only-ib" title=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-list"><line x1="8" y1="6" x2="21" y2="6"></line><line x1="8" y1="12" x2="21" y2="12"></line><line x1="8" y1="18" x2="21" y2="18"></line><line x1="3" y1="6" x2="3" y2="6"></line><line x1="3" y1="12" x2="3" y2="12"></line><line x1="3" y1="18" x2="3" y2="18"></line></svg></button><button id="menu-btn" class="hdr-btn" title=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://yinzo.github.io/posts/">Posts</a></li>
		</ul>
	</div>


	<main class="site-main section-inner animated fadeIn faster">
		<article class="thin">
			<header class="post-header">
				<div class="post-meta"><span>May 12, 2017</span></div>
				<h1>CS229 学习笔记 Part 1</h1>
			</header>
			<div class="content">
				<p>此笔记为我的 CS229 的学习笔记之一，由 Andrew Ng 的 CS229 Lecture notes 和 课堂录像整理而来。用于记录所学到的内容。记录顺序重新编排过，并非是课程原本的教学顺序，并且省略了课程中的一些推导过程，所以适合学习后整理备忘使用，不适合用于同步辅助学习。</p>
<!-- raw HTML omitted -->
<p>广义线性模型是所学到的 Linear Regression 以及 Logistic Regression 的推广形式（更准确的说，这两种模型都属于 GLM 的特殊情况）。它有三个关键假设(Assumptions)构成:</p>
<ol>
<li>$y \mid x;\theta\sim ExponentialFamily(\eta)$ ：对于固定的参数 $\theta$ 以及给定 $x$， $y$ 的分布服从某一指数分布族（如高斯分布、伯努利分布、Softmax分布）</li>
<li>对于给定的 $x$ ，目标是预测 $T(y)$ 的值。换一种说法就是，我们定义假设函数 $h(x) = E[y\mid x]$</li>
<li>natural parameter $\eta$ 和 输入 $x$ 是线性相关的， $\eta = \theta^ \mathrm{ T } x$ （其中，当输入 $x$ 和 $\eta$ 是向量的时候， $\eta_i = \theta_i^ \mathrm{T}x$）</li>
</ol>
<p>以上三个假设，一般只有第一个需要我们决定所使用的分布，其他两个假设都是直接定义。关键的地方来了，通过选择不同的_指数分布族_分布，我们能够得到不同的模型：</p>
<ul>
<li>高斯分布，则得到 Linear Regression</li>
<li>伯努利分布，则得到 Logistic Regression</li>
<li>Softmax 分布，得到 Softmax Regression</li>
</ul>
<p>其中，Lenear Regression 为回归模型 (regression)， Logistic Regression 和 Softmax Regression 都是分类模型 (classification)。</p>
<p>则我猜测，是否根据所假设的分布是离散分布还是连续分布，分别能够得到分类模型和回归模型呢？有待后续的学习验证。</p>
<!-- raw HTML omitted -->
<ol>
<li>假设 $y$ 服从的分布，并将所假设的分布化为标准的指数分布族标准形式：$$ p(y;\eta) = b(y)\exp(\eta^\mathrm{T}T(y)-a(\eta))$$</li>
<li>使用 $\eta$ 表示出 $E[y\mid x;\theta]$，则我们得到了使用 $\eta$ 表示的假设函数 $h_\theta(\eta)$</li>
<li>根据假设3，$\eta = \theta^ \mathrm{ T } x$，直接代入假设函数，得到 $h_\theta(x)$</li>
</ol>
<p>之后：</p>
<ol>
<li>通过得到的假设函数，我们能够定义出代价函数 (Cost function) 或 似然函数 (likelihood)</li>
<li>通过优化方法如梯度下降、牛顿法，以最小化代价函数或者最大化似然函数，得到最优的 $\theta$ 值。</li>
</ol>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>包括以下三种子类：</p>
<ul>
<li>Batch Gradient Descent</li>
<li>Mini-batch Gradient Descent</li>
<li>Stochastic Gradient Descent</li>
</ul>
<p>三种方法的主要区别在于，每一步优化所使用的样本数量大小：</p>
<ul>
<li>Batch 使用__全部__样本计算平均梯度后才进行一步更新；</li>
<li>Mini-batch 使用__一小部分__样本计算平均梯度就进行一步更新；</li>
<li>Stochastic 使用__一个__样本，计算梯度，进行更新</li>
</ul>
<!-- raw HTML omitted -->
<p>牛顿法用于寻找函数零点位置的优化方法，使用函数的导数（梯度）来计算参数的变化量，从而更新参数</p>
<p>由于我们需要寻找函数的极大值（极小值），则我们可以通过牛顿法寻找原函数的一阶导数的零点，将优化问题转化为寻找零点问题，以此来寻找原函数的极大值（极小值）。</p>
<p>牛顿法在二维的参数更新公式为：</p>
<p>$$\theta := \theta - \Delta$$
$$\Delta = \frac{f^\prime(\theta)}{f^{\prime\prime}(\theta)}$$</p>
<p>多维时，参数更新公式变为：</p>
<p>$$\theta := \theta - H^{-1}\nabla_\theta \ell(\theta)$$</p>
<!-- raw HTML omitted -->
<p>简单的模型介绍就不说了，只记录这门课程给我额外学习到的一些知识点。</p>
<p>首先是 Linear Regression 使用的是最小平方误差法 LMS 来进行函数优化并寻找最优 $\theta$ 值的。在我第一次学习到这个方法的时候，仅仅是觉得这个误差函数很有道理，感觉能说得过去，并没有理解其背后的理论基础。</p>
<p>Andrew Ng 使用概率学上的解释，告诉了我，为什么使用最小平方误差法。</p>
<p>我们使用假设函数 $h(x) = \theta^\mathrm{T}x^{(i)}$ 来作为给定输入 $x$ 时，对 $y$ 的预测，则我们可以得到这样一个等式</p>
<p>$$y^{(i)} = \theta^\mathrm{T}x^{(i)} + \epsilon^{(i)}$$</p>
<p>$\epsilon^{(i)}$ 是误差项，包括了未被找到的特征或随机噪声。此时，我们假设：</p>
<ol>
<li>误差都是独立同分布的</li>
<li>误差项符合高斯分布</li>
</ol>
<p>则有$\epsilon^{(i)}\sim \cal{N}(0,\sigma^2) $，即 $y^{(i)} - \theta^\mathrm{T}x^{(i)}\sim \cal{N}(0,\sigma^2) $</p>
<p>省略一部分推导，我们能够得到对数似然函数</p>
<p>$$\ell (\theta) = m\log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}\cdot\frac{1}{2}\sum_{i=1}^{m} (y^{(i)} - \theta^\mathrm{T}x^{(i)})^2$$</p>
<p>则当我们最大化对数似然函数时，即最小化</p>
<p>$$\frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^\mathrm{T}x^{(i)})^2$$</p>
<p>即最小化平方误差。</p>
<p>最终我们得到结论：当我们对数据的分布进行以上假设时，最大化对数似然函数相当于最小化平方误差。</p>
<!-- raw HTML omitted -->
<p>这是一种 Linear Regression 的特殊形式，它在误差函数中增加了一项权重，变为$\sum_i w^{(i)}(y^{(i)} - \theta^\mathrm{T}x^{(i)})^2$</p>
<p>其中 $w^{(i)}$ 就是非负权重，对于需要重点拟合的区域 $w^{(i)}$ 会比较大。$w^{(i)}​$ 并不需要满足和为1的约束，因为他不是概率分布，只是单纯的惩罚项。</p>
<p>通常来说，对于权重 $w^{(i)}​$ 的选择，我们使用以下函数</p>
<p>$$w^{(i)} = \exp(-\frac{(x^{(i)} - x)^2}{2\tau^2})$$</p>
<p>需要注意的是，这个函数和高斯分布没有关系，仅仅是常规的钟形函数而已。参数 $\tau$ 影响了当远离所求点 $x$ 时权值下降的速度。</p>
<p>则对于权重较大的区域，所求得的曲线拟合程度较高。这一方法在预测的时候，通常将待预测区域的权重提高，然后重新拟合曲线。所以每一次预测都需要重新拟合曲线，使得它在训练样本较大时效率比较低。</p>

			</div>
			<hr class="post-end">
			<footer class="post-info">
				<p>
					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://yinzo.github.io/tags/machine-learning">Machine Learning</a></span>
				</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg></p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2017-05-12 23:14 &#43;0800</p>
			</footer>
		</article>
		<aside id="toc">
			<div class="toc-title"></div>
			<nav id="TableOfContents"></nav>
		</aside>
		<div class="post-nav thin">
			<a class="next-post" href="https://yinzo.github.io/posts/cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part-2/">
				<span class="post-nav-label"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>&nbsp;</span><br><span>CS229 学习笔记 Part 2</span>
			</a>
			<a class="prev-post" href="https://yinzo.github.io/posts/%E5%8E%9F%E5%A7%8B%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/">
				<span class="post-nav-label">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg></span><br><span>原始模型优化笔记</span>
			</a>
		</div>
		<div id="comments" class="thin">
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<p id="https://yinzo.github.io/" class="leancloud_visitors" data-flag-title="雪地">
    <span class="post-meta-item-text">阅读量 </span>
    <span class="leancloud-visitors-count">-</span>
</p>
<div id="vcomments" sty ></div>
<style>
.v .vbtn {
    background: transparent !important;
    border: 2px solid #c6cddb !important;
    color: #c6cddb !important;
}

.v .vinfo .vcount .vnum {
    color: #c6cddb !important;
}

.v .vinfo .col {
    color: #c6cddb !important;
}

.v .vlist .vcard .vh .vmeta .vat {
    color: #c6cddb !important;
}

.v p, .v strong {
    color: #c6cddb !important;
}

.v code {
    font-weight: bold !important;
    font-size: 0.8rem !important;
}

.v pre {
    font-size: 1rem !important;
    font-weight: bold !important;
    background: #2c3e50 !important;
}

.v li {
    color: #c6cddb !important;
    margin-left: .5rem !important;
}

.v a {
    color: #1abc9c !important;
}

.v .vwrap .vheader .vinput {
    border-bottom: 1px dashed #c6cddb !important;
}

.v .vwrap {
    border: 2px solid #c6cddb !important;
}

.v .vinput {
    color: #c6cddb !important;
}

.v .veditor {
    color: #c6cddb !important;
}

.v .vwrap .vedit .vctrl span {
    color: #c6cddb !important;
}

.v .vwrap .vcontrol .col svg {
    color: #c6cddb !important;
}

.v .vlist .vcard .vhead .vsys {
    background: transparent !important;
    border: 1px solid #c6cddb!important;
    padding: 0 .4rem 0 .4rem !important;
    color: #c6cddb !important;
}

.v .vlist .vcard .vquote {
    border-left: 0px !important;
}

.v .vlist .vcard .vh {
    border-bottom: 0px !important;
}

.v .power {
    color: #c6cddb !important;
}

.v .vlist .vcard .vh .vtime {
    color: #c6cddb !important;
}

.v .vlist .vcard .vcontent {
    color: #c6cddb !important;
}

::-moz-placeholder { color: #ccc; }
::-webkit-input-placeholder { color:#ccc; }
:-ms-input-placeholder { color:#ccc; }
</style>
<script>    
    new Valine({
        el:'#vcomments',
        appId: 'wNSjixVlMTvSPXjMubNsdiy5-9Nh9j0Va',
        appKey: 'ygUBd8larhetzhyeV3sLpQ5q',
        visitor: true
    })
</script></div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2020 <a href="https://yinzo.github.io/">Yinzo</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://yinzo.github.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
	</footer>



	<script src="https://yinzo.github.io/js/bundle.min.4a9a0ac3d2217822c7865b4161e6c2a71de1d70492264337755427898dd718f6.js" integrity="sha256-SpoKw9IheCLHhltBYebCpx3h1wSSJkM3dVQniY3XGPY=" crossorigin="anonymous"></script>
	

</body>

</html>
